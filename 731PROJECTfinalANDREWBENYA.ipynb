{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0231d9e8",
   "metadata": {},
   "source": [
    "# Andrew Benya"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cff558",
   "metadata": {},
   "source": [
    "1. import csv file\n",
    "2. loop over all URLs in csv file\n",
    "- access website\n",
    "- get relevant text\n",
    "3. Compile all data into one corpus\n",
    "4. tokenize the text\n",
    "5. remove punctuation & numbers \n",
    "        (tests where I kept numbers & dates\n",
    "           were less useful )\n",
    "6. Lemmatize the text to gather related words\n",
    "7. Find the most frequent 3-token phrases\n",
    "8. Calculate the support of each frequent phrase\n",
    "- Repeat for 2-token\n",
    "- Repeat for 1-token\n",
    "9. Aggreate data into DataFrame\n",
    "10. Export Dataframe to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5deb4508",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fa8dafb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_letter_only(word):\n",
    "    for char in word:\n",
    "        if not char.isalpha():\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def is_alnum_only(word):\n",
    "    for char in word:\n",
    "        if not char.isalnum():\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def truncate(number, decimals=0):\n",
    "    \"\"\"\n",
    "    Returns a value truncated to a specific number of decimal places.\n",
    "    \"\"\"\n",
    "    if not isinstance(decimals, int):\n",
    "        raise TypeError(\"decimal places must be an integer.\")\n",
    "    elif decimals < 0:\n",
    "        raise ValueError(\"decimal places has to be 0 or more.\")\n",
    "    elif decimals == 0:\n",
    "        return math.trunc(number)\n",
    "\n",
    "    factor = 10.0 ** decimals\n",
    "    return math.trunc(number * factor) / factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3123a820",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "                \"Accept\":\"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\",\n",
    "                \"Accept-Encoding\":\"gzip, deflate, br\",\n",
    "                \"Accept-Language\":\"en-US,en;q=0.9\",\n",
    "                \"Connection\":\"keep-alive\",\n",
    "                \"User-Agent\":\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.119 Safari/537.36\",\n",
    "                \"Cache-Control\":\"max-age=0, no-cache, no-store\",\n",
    "                \"Upgrade-Insecure-Requests\": \"1\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7cde7a",
   "metadata": {},
   "source": [
    "# Hard coded file location here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "711dd012",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data URLs</th>\n",
       "      <th>Frequent 1-Word Set</th>\n",
       "      <th>1-Word Support</th>\n",
       "      <th>Frequent 2-Word Set</th>\n",
       "      <th>2-Word Support</th>\n",
       "      <th>Frequent 3-Word Set</th>\n",
       "      <th>3-Word Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://solarsystem.nasa.gov/resources/1063/oc...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://solarsystem.nasa.gov/resources/1064/oc...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://solarsystem.nasa.gov/resources/1065/ma...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://solarsystem.nasa.gov/resources/1066/de...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://solarsystem.nasa.gov/resources/1067/fr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Data URLs  Frequent 1-Word Set  \\\n",
       "0  https://solarsystem.nasa.gov/resources/1063/oc...                  NaN   \n",
       "1  https://solarsystem.nasa.gov/resources/1064/oc...                  NaN   \n",
       "2  https://solarsystem.nasa.gov/resources/1065/ma...                  NaN   \n",
       "3  https://solarsystem.nasa.gov/resources/1066/de...                  NaN   \n",
       "4  https://solarsystem.nasa.gov/resources/1067/fr...                  NaN   \n",
       "\n",
       "   1-Word Support  Frequent 2-Word Set  2-Word Support  Frequent 3-Word Set  \\\n",
       "0             NaN                  NaN             NaN                  NaN   \n",
       "1             NaN                  NaN             NaN                  NaN   \n",
       "2             NaN                  NaN             NaN                  NaN   \n",
       "3             NaN                  NaN             NaN                  NaN   \n",
       "4             NaN                  NaN             NaN                  NaN   \n",
       "\n",
       "   3-Word Support  \n",
       "0             NaN  \n",
       "1             NaN  \n",
       "2             NaN  \n",
       "3             NaN  \n",
       "4             NaN  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()\n",
    "## HARD CODING WARNING\n",
    "os.chdir('C:\\\\Users\\\\andre\\\\Documents\\\\BOWIE\\\\COSC 731')\n",
    "os.getcwd()\n",
    "df = pd.read_csv('Frequent Keywords.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2943275e",
   "metadata": {},
   "outputs": [],
   "source": [
    "weirdURL = 'https://solarsystem.nasa.gov/resources/1063/occator-crater-on-ceres-limb-short-exposure/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5a3af0",
   "metadata": {},
   "source": [
    "## i had a tough time getting the request to work the first time.  \n",
    "### all 546 other URLs connected on the first try, buit not the first one.\n",
    "### i think there was some error in the URL or something, i had to hard code it to the same address\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cee901f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = requests.get(weirdURL,headers=headers)\n",
    "w.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1a110bae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the first URL was not the same as the URL from the brower\n",
    "df['Data URLs'][0] == weirdURL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "68d86603",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# So i just replaced it here, and it fixed the request\n",
    "df.at[0,'Data URLs'] = weirdURL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "84e638ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "URLS = []\n",
    "index = 0\n",
    "\n",
    "for item in df['Data URLs']:\n",
    "    URLS.append(item)\n",
    "    #print(item)\n",
    "    \n",
    "for URL in URLS:\n",
    "    #print(URL)\n",
    "    r = requests.get(URL,headers=headers)\n",
    "    #print(\"URL # {}, Status Code:{}\".format(index, r.status_code))\n",
    "    if r.status_code == requests.codes.ok:\n",
    "        soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "        results = soup.find_all(class_=\"content_page module\")\n",
    "        results[0].text.strip()\n",
    "        document = results[0].text.replace(\"\\n\",\" \")\n",
    "        \n",
    "    else:\n",
    "        # this actually breaks the support calulation if \n",
    "        # you have any empty documents \n",
    "        # So all inputs need to work\n",
    "        document = ' '\n",
    "        \n",
    "    corpus.append(document)\n",
    "    # debugging index\n",
    "    index += 1 \n",
    "        \n",
    "#print(len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "901d0a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out all non alpha characters\n",
    "data_cleaned_only_alpha = []\n",
    "for doc in corpus:\n",
    "    doc_cleaned = ' '.join(word for word in doc.split()\n",
    "                           if is_letter_only(word) )\n",
    "    data_cleaned_only_alpha.append(doc_cleaned)\n",
    "#print(data_cleaned_only_alpha[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0ed9ce73",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9b82b062",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaned_only_alpha_lemmed = []\n",
    "for doc in data_cleaned_only_alpha:\n",
    "    doc = doc.lower()\n",
    "    doc_cleaned = ' '.join(lemmatizer.lemmatize(word)\n",
    "                           for word in doc.split()\n",
    "                           if is_letter_only(word))\n",
    "    data_cleaned_only_alpha_lemmed.append(doc_cleaned)\n",
    "#print(data_cleaned_only_alpha_lemmed[1])\n",
    "\n",
    "## lemmatizing takes the S off of the word \"was\" ...  hmmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e650422d",
   "metadata": {},
   "outputs": [],
   "source": [
    "countVectorizer3token = CountVectorizer(ngram_range=(3,3),\n",
    "                                 max_features=500,\n",
    "                                 stop_words='english',\n",
    "                                 lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0e6d3f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "three_token_fitted = countVectorizer3token.fit_transform(data_cleaned_only_alpha_lemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3d99cc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_dict = sorted(countVectorizer3token.vocabulary_.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "08409d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "three_token_fitted_array = three_token_fitted.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8a21339a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clear Lists\n",
    "\n",
    "list_of_lists = []\n",
    "frequent_set_3_grams = []\n",
    "three_word_frequency = []\n",
    "three_word_support = []\n",
    "freq_word_indicies = []\n",
    "for i in range(len(three_token_fitted_array)):\n",
    "    frequent_set_3_grams.append([])\n",
    "    three_word_frequency.append([])\n",
    "    three_word_support.append([])\n",
    "    freq_word_indicies.append([])\n",
    "    list_of_lists.append([])\n",
    "#print(frequent_set_3_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "644d33cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for y in range(0,len(three_token_fitted_array)):\n",
    "    for x in range(0,len(three_token_fitted_array[y])):\n",
    "        if three_token_fitted_array[y][x] != 0:\n",
    "            #print(y,x)\n",
    "            freq_word_indicies[y].append(x)\n",
    "            frequent_set_3_grams[y].append(feat_dict[x])\n",
    "            three_word_frequency[y].append(three_token_fitted_array[y][x])\n",
    "            \n",
    "#print(len(frequent_set_3_grams))\n",
    "#print(len(freq_word_indicies))\n",
    "#print(len(three_word_frequency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b14146d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for x in range(0,len(three_word_frequency)):\n",
    "    sum = 0\n",
    "    for y in range(0,len(three_word_frequency[x])):\n",
    "        sum += three_word_frequency[x][y]\n",
    "        \n",
    "    for y in range(0,len(three_word_frequency[x])): \n",
    "        #print(x,y)\n",
    "        support = three_word_frequency[x][y] / sum\n",
    "        three_word_support[x].append(truncate(support, 4))\n",
    "\n",
    "        \n",
    "#print(three_word_support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "076f8a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "countVectorizer2token = CountVectorizer(ngram_range=(2,2),\n",
    "                                 max_features=500,\n",
    "                                 stop_words='english',\n",
    "                                 lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "751d128e",
   "metadata": {},
   "outputs": [],
   "source": [
    "two_token_fitted = countVectorizer2token.fit_transform(data_cleaned_only_alpha_lemmed)\n",
    "feat_dict_2_token = sorted(countVectorizer2token.vocabulary_.keys())\n",
    "two_token_fitted_array = two_token_fitted.toarray()\n",
    "#Clear Lists\n",
    "\n",
    "list_of_lists = []\n",
    "frequent_set_2_grams = []\n",
    "two_word_frequency = []\n",
    "two_word_support = []\n",
    "freq_two_word_indicies = []\n",
    "for i in range(len(two_token_fitted_array)):\n",
    "    frequent_set_2_grams.append([])\n",
    "    two_word_frequency.append([])\n",
    "    two_word_support.append([])\n",
    "    freq_two_word_indicies.append([])\n",
    "    list_of_lists.append([])\n",
    "#print(frequent_set_3_grams)\n",
    "\n",
    "for y in range(0,len(two_token_fitted_array)):\n",
    "    for x in range(0,len(two_token_fitted_array[y])):\n",
    "        if two_token_fitted_array[y][x] != 0:\n",
    "            #print(y,x)\n",
    "            freq_two_word_indicies[y].append(x)\n",
    "            frequent_set_2_grams[y].append(feat_dict_2_token[x])\n",
    "            two_word_frequency[y].append(two_token_fitted_array[y][x])\n",
    "            \n",
    "#print(frequent_set_2_grams)\n",
    "#print(len(freq_two_word_indicies))\n",
    "#print(len(two_word_frequency))\n",
    "\n",
    "for x in range(0,len(two_word_frequency)):\n",
    "    sum = 0\n",
    "    for y in range(0,len(two_word_frequency[x])):\n",
    "        sum += two_word_frequency[x][y]\n",
    "        \n",
    "    for y in range(0,len(two_word_frequency[x])): \n",
    "        #print(x,y)\n",
    "        support = two_word_frequency[x][y] / sum\n",
    "        two_word_support[x].append(truncate(support, 4))\n",
    "\n",
    "#print(\"two word support\")        \n",
    "#print(two_word_support)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cac10e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "countVectorizer1token = CountVectorizer(ngram_range=(1,1),\n",
    "                                 max_features=500,\n",
    "                                 stop_words='english',\n",
    "                                 lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1df6dc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_token_fitted = countVectorizer1token.fit_transform(data_cleaned_only_alpha_lemmed)\n",
    "feat_dict_1_token = sorted(countVectorizer1token.vocabulary_.keys())\n",
    "one_token_fitted_array = one_token_fitted.toarray()\n",
    "#Clear Lists\n",
    "\n",
    "list_of_lists = []\n",
    "frequent_set_1_grams = []\n",
    "one_word_frequency = []\n",
    "one_word_support = []\n",
    "freq_one_word_indicies = []\n",
    "for i in range(len(one_token_fitted_array)):\n",
    "    frequent_set_1_grams.append([])\n",
    "    one_word_frequency.append([])\n",
    "    one_word_support.append([])\n",
    "    freq_one_word_indicies.append([])\n",
    "    list_of_lists.append([])\n",
    "#print(frequent_set_3_grams)\n",
    "\n",
    "for y in range(0,len(one_token_fitted_array)):\n",
    "    for x in range(0,len(one_token_fitted_array[y])):\n",
    "        if one_token_fitted_array[y][x] != 0:\n",
    "            #print(y,x)\n",
    "            freq_one_word_indicies[y].append(x)\n",
    "            frequent_set_1_grams[y].append(feat_dict_1_token[x])\n",
    "            one_word_frequency[y].append(one_token_fitted_array[y][x])\n",
    "            \n",
    "#print(frequent_set_1_grams)\n",
    "#print(len(freq_one_word_indicies))\n",
    "#print(len(one_word_frequency))\n",
    "\n",
    "for x in range(0,len(one_word_frequency)):\n",
    "    sum = 0\n",
    "    for y in range(0,len(one_word_frequency[x])):\n",
    "        sum += one_word_frequency[x][y]\n",
    "        \n",
    "    for y in range(0,len(one_word_frequency[x])): \n",
    "        #print(x,y)\n",
    "        support = one_word_frequency[x][y] / sum\n",
    "        one_word_support[x].append(truncate(support, 4))\n",
    "\n",
    "#print(\"one word support\")        \n",
    "#print(one_word_support)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "41ca627f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DataURLS</th>\n",
       "      <th>frequent 1-word set</th>\n",
       "      <th>1-word support</th>\n",
       "      <th>frequent 2-word set</th>\n",
       "      <th>2-word support</th>\n",
       "      <th>frequent 3-word set</th>\n",
       "      <th>3-word support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://solarsystem.nasa.gov/resources/1063/oc...</td>\n",
       "      <td>[altitude, august, cere, crater, dawn, enlarge...</td>\n",
       "      <td>[0.04, 0.04, 0.08, 0.08, 0.04, 0.04, 0.08, 0.0...</td>\n",
       "      <td>[altitude mile, crater cere, dawn spacecraft, ...</td>\n",
       "      <td>[0.0666, 0.0666, 0.0666, 0.0666, 0.0666, 0.066...</td>\n",
       "      <td>[altitude mile enlarge, dawn spacecraft august...</td>\n",
       "      <td>[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://solarsystem.nasa.gov/resources/1064/oc...</td>\n",
       "      <td>[altitude, august, cere, crater, dawn, enlarge...</td>\n",
       "      <td>[0.04, 0.04, 0.08, 0.08, 0.04, 0.04, 0.08, 0.0...</td>\n",
       "      <td>[altitude mile, crater cere, dawn spacecraft, ...</td>\n",
       "      <td>[0.0666, 0.0666, 0.0666, 0.0666, 0.0666, 0.066...</td>\n",
       "      <td>[altitude mile enlarge, dawn spacecraft august...</td>\n",
       "      <td>[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://solarsystem.nasa.gov/resources/1065/ma...</td>\n",
       "      <td>[altitude, august, cere, crater, dawn, enlarge...</td>\n",
       "      <td>[0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.08, 0.0...</td>\n",
       "      <td>[altitude mile, crater rim, dawn spacecraft, e...</td>\n",
       "      <td>[0.0666, 0.0666, 0.0666, 0.0666, 0.1333, 0.066...</td>\n",
       "      <td>[altitude mile enlarge, dawn spacecraft august...</td>\n",
       "      <td>[0.1111, 0.1111, 0.1111, 0.1111, 0.1111, 0.111...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://solarsystem.nasa.gov/resources/1066/de...</td>\n",
       "      <td>[altitude, cere, crater, dawn, enlarge, fractu...</td>\n",
       "      <td>[0.05, 0.05, 0.1, 0.05, 0.05, 0.1, 0.05, 0.05,...</td>\n",
       "      <td>[altitude mile, crater september, crater wa, d...</td>\n",
       "      <td>[0.0588, 0.0588, 0.0588, 0.0588, 0.0588, 0.117...</td>\n",
       "      <td>[altitude mile enlarge, crater september image...</td>\n",
       "      <td>[0.0666, 0.0666, 0.0666, 0.0666, 0.0666, 0.133...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://solarsystem.nasa.gov/resources/1067/fr...</td>\n",
       "      <td>[altitude, cere, crater, dawn, enlarge, floor,...</td>\n",
       "      <td>[0.0416, 0.0416, 0.0833, 0.0416, 0.0416, 0.083...</td>\n",
       "      <td>[altitude mile, crater september, crater wa, d...</td>\n",
       "      <td>[0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.095...</td>\n",
       "      <td>[altitude mile enlarge, crater september image...</td>\n",
       "      <td>[0.0588, 0.0588, 0.0588, 0.0588, 0.0588, 0.117...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            DataURLS  \\\n",
       "0  https://solarsystem.nasa.gov/resources/1063/oc...   \n",
       "1  https://solarsystem.nasa.gov/resources/1064/oc...   \n",
       "2  https://solarsystem.nasa.gov/resources/1065/ma...   \n",
       "3  https://solarsystem.nasa.gov/resources/1066/de...   \n",
       "4  https://solarsystem.nasa.gov/resources/1067/fr...   \n",
       "\n",
       "                                 frequent 1-word set  \\\n",
       "0  [altitude, august, cere, crater, dawn, enlarge...   \n",
       "1  [altitude, august, cere, crater, dawn, enlarge...   \n",
       "2  [altitude, august, cere, crater, dawn, enlarge...   \n",
       "3  [altitude, cere, crater, dawn, enlarge, fractu...   \n",
       "4  [altitude, cere, crater, dawn, enlarge, floor,...   \n",
       "\n",
       "                                      1-word support  \\\n",
       "0  [0.04, 0.04, 0.08, 0.08, 0.04, 0.04, 0.08, 0.0...   \n",
       "1  [0.04, 0.04, 0.08, 0.08, 0.04, 0.04, 0.08, 0.0...   \n",
       "2  [0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.08, 0.0...   \n",
       "3  [0.05, 0.05, 0.1, 0.05, 0.05, 0.1, 0.05, 0.05,...   \n",
       "4  [0.0416, 0.0416, 0.0833, 0.0416, 0.0416, 0.083...   \n",
       "\n",
       "                                 frequent 2-word set  \\\n",
       "0  [altitude mile, crater cere, dawn spacecraft, ...   \n",
       "1  [altitude mile, crater cere, dawn spacecraft, ...   \n",
       "2  [altitude mile, crater rim, dawn spacecraft, e...   \n",
       "3  [altitude mile, crater september, crater wa, d...   \n",
       "4  [altitude mile, crater september, crater wa, d...   \n",
       "\n",
       "                                      2-word support  \\\n",
       "0  [0.0666, 0.0666, 0.0666, 0.0666, 0.0666, 0.066...   \n",
       "1  [0.0666, 0.0666, 0.0666, 0.0666, 0.0666, 0.066...   \n",
       "2  [0.0666, 0.0666, 0.0666, 0.0666, 0.1333, 0.066...   \n",
       "3  [0.0588, 0.0588, 0.0588, 0.0588, 0.0588, 0.117...   \n",
       "4  [0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.095...   \n",
       "\n",
       "                                 frequent 3-word set  \\\n",
       "0  [altitude mile enlarge, dawn spacecraft august...   \n",
       "1  [altitude mile enlarge, dawn spacecraft august...   \n",
       "2  [altitude mile enlarge, dawn spacecraft august...   \n",
       "3  [altitude mile enlarge, crater september image...   \n",
       "4  [altitude mile enlarge, crater september image...   \n",
       "\n",
       "                                      3-word support  \n",
       "0  [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, ...  \n",
       "1  [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, ...  \n",
       "2  [0.1111, 0.1111, 0.1111, 0.1111, 0.1111, 0.111...  \n",
       "3  [0.0666, 0.0666, 0.0666, 0.0666, 0.0666, 0.133...  \n",
       "4  [0.0588, 0.0588, 0.0588, 0.0588, 0.0588, 0.117...  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {'DataURLS':df['Data URLs'],\n",
    "       'frequent 1-word set': frequent_set_1_grams,\n",
    "        '1-word support': one_word_support,\n",
    "        'frequent 2-word set': frequent_set_2_grams,\n",
    "        '2-word support': two_word_support,\n",
    "        'frequent 3-word set': frequent_set_3_grams,\n",
    "        '3-word support': three_word_support,\n",
    "       }\n",
    "\n",
    "\n",
    "newDF = pd.DataFrame(data)\n",
    "newDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "354641d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "newDF.to_csv('731projectANDREWBENYA.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a5f442",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
